{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Implement a simple rule-based Text tokenizer for the English language using regular expressions. Your tokenizer should consider punctuations and special symbols as separate tokens. Contractions like \"isn't\" should be regarded as 2 tokens - \"is\" and \"n't\". Also identify abbreviations (eg, U.S.A) and internal hyphenation (eg. ice-cream), as single tokens."
      ],
      "metadata": {
        "id": "nI3K-i2lLX5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def tokenize(text):\n",
        "    # Updated pattern to match abbreviations, contractions, words, and special symbols.\n",
        "    pattern = r'''\n",
        "        \\b(?:[A-Za-z]\\.){2,}[A-Za-z]?   # Match abbreviations like U.S.A\n",
        "        | \\b\\w+'\\w+\\b                   # Match contractions like isn't, can't\n",
        "        | \\b\\w+(?:-\\w+)+\\b              # Match hyphenated words like ice-cream\n",
        "        | \\b\\w+\\b                       # Match regular words\n",
        "        | [.,!?;:'\"(){}\\[\\]<>@#$%^&*-+=~`|\\\\/]  # Match individual punctuation and special characters\n",
        "    '''\n",
        "    # Compile pattern with VERBOSE flag to allow comments\n",
        "    tokenizer = re.compile(pattern, re.VERBOSE)\n",
        "\n",
        "    # Find all tokens based on the pattern\n",
        "    tokens = tokenizer.findall(text)\n",
        "\n",
        "    # Use a set to eliminate duplicate tokens\n",
        "    unique_tokens = set()\n",
        "\n",
        "    # Post-process to further split contractions and hyphenated words if needed\n",
        "    for token in tokens:\n",
        "        # Split contractions like \"isn't\" into 'is' and \"n't\"\n",
        "        if re.match(r\"\\b\\w+'(?:t|ll|re|ve|d|m|s)\\b\", token):  # Matches common English contractions\n",
        "            unique_tokens.add(token[:-3] if token.endswith(\"n't\") else token[:-2])  # Base word\n",
        "            unique_tokens.add(token[-3:] if token.endswith(\"n't\") else token[-2:])  # Contraction part\n",
        "        elif '-' in token and not token.startswith('-') and not token.endswith('-'):  # Split hyphenated words\n",
        "            split_parts = token.split('-')\n",
        "            for part in split_parts[:-1]:\n",
        "                unique_tokens.add(part)\n",
        "                unique_tokens.add('-')\n",
        "            unique_tokens.add(split_parts[-1])  # Add the last part without adding another hyphen\n",
        "        else:\n",
        "            unique_tokens.add(token)\n",
        "\n",
        "    # Convert set back to list and sort for consistent order\n",
        "    unique_tokens_list = sorted(unique_tokens)\n",
        "\n",
        "    # Create a list of lists for the table\n",
        "    table = [[i, unique_tokens_list[i]] for i in range(len(unique_tokens_list))]\n",
        "    return table\n",
        "\n",
        "# Example usage\n",
        "text = \"The U.K. has world-class museums, doesn't it? I can't wait to visit! Ice-cream and fun-filled adventures await.\"\n",
        "tokens_table = tokenize(text)\n",
        "\n",
        "# Print the table\n",
        "print(\"Index\\tToken\")\n",
        "for index, token in tokens_table:\n",
        "    print(f\"{index}\\t{token}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4VwHDnI4e4i",
        "outputId": "806636aa-839f-4e89-8cfa-4f557d4ff56c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index\tToken\n",
            "0\t!\n",
            "1\t,\n",
            "2\t-\n",
            "3\t.\n",
            "4\t?\n",
            "5\tI\n",
            "6\tIce\n",
            "7\tThe\n",
            "8\tU.K.\n",
            "9\tadventures\n",
            "10\tand\n",
            "11\tawait\n",
            "12\tca\n",
            "13\tclass\n",
            "14\tcream\n",
            "15\tdoes\n",
            "16\tfilled\n",
            "17\tfun\n",
            "18\thas\n",
            "19\tit\n",
            "20\tmuseums\n",
            "21\tn't\n",
            "22\tto\n",
            "23\tvisit\n",
            "24\twait\n",
            "25\tworld\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Design and implement a Finite State Automata(FSA) that accepts English plural nouns ending with the character  ‘y’, e.g. boys, toys, ponies, skies, and puppies but not boies or toies or ponys. (Hint: Words that end with a vowel followed by ‘y’ are appended with ‘s' and will not be replaced with “ies” in their plural form).\n"
      ],
      "metadata": {
        "id": "IUmsAJbwLg9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_vowel(char):\n",
        "    return char.lower() in 'aeiou'\n",
        "\n",
        "def check_plural_noun(word):\n",
        "    if len(word) < 2:\n",
        "        return False  # A plural noun ending in 'y' must have at least two characters\n",
        "\n",
        "    # Transition through states based on the last two or three letters of the word\n",
        "    if word.endswith(\"ies\"):\n",
        "        # Check for a consonant before 'y'\n",
        "        if not is_vowel(word[-4]) and word[-4].isalpha():\n",
        "            return True  # Matches consonant + \"y\" → \"ies\" rule\n",
        "    elif word.endswith(\"s\") and word[-2] == \"y\":\n",
        "        # Check for a vowel before 'y'\n",
        "        if is_vowel(word[-3]):\n",
        "            return True  # Matches vowel + \"y\" → \"ys\" rule\n",
        "\n",
        "    return False\n",
        "\n",
        "# Test cases\n",
        "test_words = [\"boys\", \"toys\", \"ponies\", \"skies\", \"puppies\", \"boies\", \"toies\", \"ponys\"]\n",
        "results = {word: check_plural_noun(word) for word in test_words}\n",
        "\n",
        "# Output results\n",
        "for word, result in results.items():\n",
        "    print(f\"{word}: {'Accepted' if result else 'Rejected'}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3mbIz-gPyhi",
        "outputId": "9ab3f76d-8dfe-4dca-ce0a-de570f8b7100"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boys: Accepted\n",
            "toys: Accepted\n",
            "ponies: Accepted\n",
            "skies: Accepted\n",
            "puppies: Accepted\n",
            "boies: Rejected\n",
            "toies: Rejected\n",
            "ponys: Rejected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_plural_noun_accepted_fsa(word):\n",
        "    # Check if the word is too short or doesn't end with 's'\n",
        "    if len(word) < 2 or word[-1] != 's':\n",
        "        return False\n",
        "\n",
        "    # Reverse the word for processing\n",
        "    word = word[::-1]\n",
        "    state = 'S1'\n",
        "\n",
        "    for char in word[1:]:\n",
        "        if state == 'S1':\n",
        "            if char == 'y':\n",
        "                state = 'S2'\n",
        "            elif char == 'e':\n",
        "                state = 'S3'\n",
        "            else:\n",
        "                return False\n",
        "        elif state == 'S2':\n",
        "            if char in 'aeiou':\n",
        "                state = 'S5'\n",
        "            else:\n",
        "                return False\n",
        "        elif state == 'S3':\n",
        "            if char == 'i':\n",
        "                state = 'S4'\n",
        "            else:\n",
        "                return False\n",
        "        elif state == 'S4':\n",
        "            if char.isalpha() and char not in 'aeiou':\n",
        "                state = 'S6'\n",
        "            else:\n",
        "                return False\n",
        "        elif state == 'S5':\n",
        "            continue  # Stay in S5 if we encounter a vowel\n",
        "        elif state == 'S6':\n",
        "            continue  # Stay in S6 if we encounter a consonant\n",
        "\n",
        "    return True\n",
        "\n",
        "# List of test words\n",
        "test_words = ['ladies' ,'boys', 'toys', 'ponies', 'skies', 'puppies', 'boies', 'toies', 'ponys', 'carries', 'daisies']\n",
        "\n",
        "# Create a dictionary to store the results of the test words\n",
        "results = {word: is_plural_noun_accepted_fsa(word) for word in test_words}\n",
        "\n",
        "# Print the results\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuJpsTYJkVPH",
        "outputId": "f5ff1ac6-42fd-49f0-c363-8f1aed5a2efb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ladies': True, 'boys': True, 'toys': True, 'ponies': True, 'skies': True, 'puppies': True, 'boies': False, 'toies': False, 'ponys': False, 'carries': True, 'daisies': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from graphviz import Digraph\n",
        "\n",
        "# Create a directed graph\n",
        "dot = Digraph()\n",
        "\n",
        "# Add nodes for states\n",
        "dot.node('S1', 'S1: Start', shape='ellipse')\n",
        "dot.node('S2', 'S2: After \\'y\\'', shape='ellipse')\n",
        "dot.node('S3', 'S3: After \\'e\\'', shape='ellipse')\n",
        "dot.node('S4', 'S4: After \\'i\\'', shape='ellipse')\n",
        "dot.node('S5', 'S5: Vowel state', shape='ellipse')\n",
        "dot.node('S6', 'S6: Consonant state', shape='ellipse')\n",
        "dot.node('R', 'Reject', shape='ellipse', color='red')\n",
        "\n",
        "# Add edges for transitions\n",
        "dot.edge('S1', 'S2', 'y')\n",
        "dot.edge('S1', 'S3', 'e')\n",
        "dot.edge('S1', 'R', 'Other')\n",
        "\n",
        "dot.edge('S2', 'S5', 'Vowel')\n",
        "dot.edge('S2', 'R', 'Consonant')\n",
        "\n",
        "dot.edge('S3', 'S4', 'i')\n",
        "dot.edge('S3', 'R', 'Other')\n",
        "\n",
        "dot.edge('S4', 'S6', 'Consonant')\n",
        "dot.edge('S4', 'R', 'Vowel')\n",
        "\n",
        "dot.edge('S5', 'S5', 'Vowel/Consonant')\n",
        "dot.edge('S6', 'S6', 'Vowel/Consonant')\n",
        "\n",
        "# Render and view the graph\n",
        "dot.render('fsa_diagram', format='png', cleanup=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "w7Fs_2w_igxB",
        "outputId": "3587429c-8b02-4b22-b552-1e632f719757"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fsa_diagram.png'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Design and implement a Finite State Transducer(FST) that accepts lexical forms of English words(e.g. shown below) and generates its corresponding plurals, based on the e-insertion spelling rule є => e / {x,s,z}^ __ s#\n",
        "^ is the morpheme boundary and # - word boundary\n"
      ],
      "metadata": {
        "id": "3AflszxILxmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pluralize_fst(word):\n",
        "    # Check for format validity\n",
        "    if '^' not in word or not word.endswith('#'):\n",
        "        return \"Invalid format\"  # Invalid input format\n",
        "\n",
        "    # Split the word into root and morpheme\n",
        "    root, morpheme = word.split('^')\n",
        "    morpheme = morpheme.rstrip('#')  # Remove the word boundary symbol\n",
        "\n",
        "    # Ensure the morpheme is \"s\" for pluralization\n",
        "    if morpheme != 's':\n",
        "        return \"Invalid morpheme\"\n",
        "\n",
        "    # Initialize the state\n",
        "    state = 'START'\n",
        "\n",
        "    # Transition through states based on the last character of the root\n",
        "    last_char = root[-1]\n",
        "\n",
        "    if state == 'START':\n",
        "        if last_char in {'x', 's', 'z'}:\n",
        "            # Transition for e-insertion rule\n",
        "            plural_form = root + \"es\"\n",
        "        else:\n",
        "            # Transition for regular pluralization\n",
        "            plural_form = root + \"s\"\n",
        "\n",
        "    return plural_form\n",
        "\n",
        "# Test cases\n",
        "test_words = [\"fox^s#\", \"boy^s#\"]\n",
        "results = {word: pluralize_fst(word) for word in test_words}\n",
        "\n",
        "# Output results\n",
        "for word, result in results.items():\n",
        "    print(f\"{word}: {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgEoKDimlrab",
        "outputId": "4a397736-8d52-4259-e327-d61b895e174a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fox^s#: foxes\n",
            "boy^s#: boys\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pluralize(word):\n",
        "    \"\"\"\n",
        "    Pluralizes an English word based on the e-insertion rule:\n",
        "    - Inserts 'e' before 's' if the word ends with x^, s^, or z^ (indicating a morpheme boundary).\n",
        "    - Otherwise, simply appends 's' to form the plural.\n",
        "\n",
        "    Args:\n",
        "    - word (str): The word in lexical form with a morpheme boundary symbol (^) at the end of the base word.\n",
        "\n",
        "    Returns:\n",
        "    - str: The pluralized form of the word.\n",
        "    \"\"\"\n",
        "    if word.endswith((\"x^\", \"s^\", \"z^\")):\n",
        "        # Rule: e-insertion for words ending in x, s, z with morpheme boundary (^)\n",
        "        base_word = word[:-1]  # Remove the morpheme boundary (^)\n",
        "        plural_word = base_word + \"es\"  # Insert 'e' before 's'\n",
        "    elif word.endswith(\"^\"):\n",
        "        # For other cases with a morpheme boundary, just add 's'\n",
        "        plural_word = word[:-1] + \"s\"  # Remove ^ and add 's'\n",
        "    else:\n",
        "        # Default case (if ^ is not present, assume it’s a regular plural form)\n",
        "        plural_word = word + \"s\"\n",
        "\n",
        "    return plural_word\n",
        "\n",
        "\n",
        "# Test cases\n",
        "test_words = [\"fox^s#\", \"boy^s#\"]\n",
        "print(\"Pluralized words:\")\n",
        "for word in test_words:\n",
        "    print(f\"{word} -> {pluralize(word)}\")\n",
        "from graphviz import Digraph\n",
        "\n",
        "# Initialize the FST diagram\n",
        "fst = Digraph(\"Finite State Transducer for Pluralization\", format=\"png\")\n",
        "fst.attr(rankdir=\"LR\")  # Left to right orientation\n",
        "\n",
        "# Define states\n",
        "fst.node(\"S0\", \"Start (S0)\")\n",
        "fst.node(\"S1\", \"Suffix Check (S1)\")\n",
        "fst.node(\"S2\", \"Insert 'e' (S2)\")\n",
        "fst.node(\"S3\", \"Final State (S3)\")\n",
        "\n",
        "# Define transitions\n",
        "fst.edge(\"S0\", \"S1\", label=\"x^, s^, z^\")  # From Start to Suffix Check\n",
        "fst.edge(\"S1\", \"S2\", label=\"Insert 'e'\")   # Insert \"e\" before \"s\"\n",
        "fst.edge(\"S2\", \"S3\", label=\"Add 's'\")      # Add \"s\" to form the plural\n",
        "fst.edge(\"S0\", \"S3\", label=\"Other endings, add 's'\")  # Direct pluralization for other cases\n",
        "\n",
        "# Save and render the diagram\n",
        "fst.render(\"pluralization_fst\")\n",
        "\n",
        "print(\"FST diagram generated and saved as 'pluralization_fst.png'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruDuuoV1gQl9",
        "outputId": "f0189cb0-09c4-4ba0-fe6b-a4a51f62453f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pluralized words:\n",
            "fox^s# -> fox^s#s\n",
            "boy^s# -> boy^s#s\n",
            "FST diagram generated and saved as 'pluralization_fst.png'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the Minimum Edit Distance algorithm to find the edit distance between any two given strings. Also, list the edit operations."
      ],
      "metadata": {
        "id": "fQ300l8EL3Ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def min_edit_distance(A, B):\n",
        "    m = len(A)\n",
        "    n = len(B)\n",
        "\n",
        "    # Create a matrix of size (m+1) x (n+1)\n",
        "    D = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "    # Initialize the first row and column\n",
        "    for i in range(m + 1):\n",
        "        D[i][0] = i  # Deletion\n",
        "    for j in range(n + 1):\n",
        "        D[0][j] = j  # Insertion\n",
        "\n",
        "    # Fill the matrix\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            if A[i - 1] == B[j - 1]:\n",
        "                D[i][j] = D[i - 1][j - 1]  # No cost\n",
        "            else:\n",
        "                D[i][j] = min(\n",
        "                    D[i - 1][j] + 1,    # Deletion\n",
        "                    D[i][j - 1] + 1,    # Insertion\n",
        "                    D[i - 1][j - 1] + 2  # Substitution\n",
        "                )\n",
        "\n",
        "    # Backtrack to find the operations\n",
        "    operations = []\n",
        "    i, j = m, n\n",
        "\n",
        "    while i > 0 or j > 0:\n",
        "        if i > 0 and j > 0 and A[i - 1] == B[j - 1]:\n",
        "            i -= 1\n",
        "            j -= 1\n",
        "        elif i > 0 and (j == 0 or D[i][j] == D[i - 1][j] + 1):\n",
        "            operations.append(f\"Delete '{A[i - 1]}' from A\")\n",
        "            i -= 1\n",
        "        elif j > 0 and (i == 0 or D[i][j] == D[i][j - 1] + 1):\n",
        "            operations.append(f\"Insert '{B[j - 1]}' to A\")\n",
        "            j -= 1\n",
        "        else:\n",
        "            operations.append(f\"Substitute '{A[i - 1]}' in A with '{B[j - 1]}'\")\n",
        "            i -= 1\n",
        "            j -= 1\n",
        "\n",
        "    operations.reverse()  # Reverse to get the order of operations from start to finish\n",
        "    return D[m][n], operations\n",
        "\n",
        "# Example usage\n",
        "A = \"Execution\"\n",
        "B = \"intention\"\n",
        "edit_distance, edit_operations = min_edit_distance(A, B)\n",
        "\n",
        "print(\"Minimum Edit Distance:\", edit_distance)\n",
        "print(\"Edit Operations:\")\n",
        "for operation in edit_operations:\n",
        "    print(operation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVFkKx-3Righ",
        "outputId": "1da87d44-0c44-4e41-e6b8-49db05dd3e0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum Edit Distance: 8\n",
            "Edit Operations:\n",
            "Insert 'i' to A\n",
            "Insert 'n' to A\n",
            "Insert 't' to A\n",
            "Delete 'E' from A\n",
            "Delete 'x' from A\n",
            "Insert 'n' to A\n",
            "Delete 'c' from A\n",
            "Delete 'u' from A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Design and implement a statistical spell checker for detecting and correcting non-word spelling errors in English, using the bigram language model. Your program should do the following:\n",
        "Tokenize the corpus and create a vocabulary of unique words.\n",
        "Create a bi-gram frequency table for all possible bigrams in the corpus.\n",
        "Scan the given input text to identify the non-word spelling errors\n",
        "Generate the candidate list using 1 edit distance from the misspelled words\n",
        "Suggest the best candidate word by calculating the probability of the given sentence using the bigram LM.\n"
      ],
      "metadata": {
        "id": "FuU1tfFIMP5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "def tokenize(text):\n",
        "    # Tokenize text into words using regex for simplicity\n",
        "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
        "\n",
        "def build_vocabulary_and_bigrams(corpus):\n",
        "    words = tokenize(corpus)\n",
        "    vocabulary = set(words)\n",
        "\n",
        "    bigram_freq = defaultdict(int)\n",
        "    for i in range(len(words) - 1):\n",
        "        bigram = (words[i], words[i+1])\n",
        "        bigram_freq[bigram] += 1\n",
        "\n",
        "    print(\"Vocabulary:\", vocabulary)  # Print the vocabulary\n",
        "    print(\"Bigram Frequency Table:\", dict(bigram_freq))  # Print bigram frequency table\n",
        "\n",
        "    return vocabulary, bigram_freq\n",
        "\n",
        "def find_misspellings(text, vocabulary):\n",
        "    words = tokenize(text)\n",
        "    misspellings = [word for word in words if word not in vocabulary]\n",
        "    print(\"Identified Misspellings:\", misspellings)  # Print identified misspellings\n",
        "    return misspellings\n",
        "\n",
        "def edits1(word):\n",
        "    # Generate all words one edit away from `word`\n",
        "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "    deletes = [L + R[1:] for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
        "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
        "    inserts = [L + c + R for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def get_candidates(word, vocabulary):\n",
        "    # Get candidate corrections for the misspelled word\n",
        "    candidates = {w for w in edits1(word) if w in vocabulary}\n",
        "    print(f\"Candidates for '{word}':\", candidates)  # Print candidate corrections\n",
        "    return candidates\n",
        "\n",
        "def get_multigram_candidates(misspelled_word, words, vocabulary):\n",
        "    # Generate multi-word candidates by combining single-word candidates\n",
        "    candidates = set()\n",
        "    for i in range(len(words)):\n",
        "        for j in range(i + 1, len(words) + 1):\n",
        "            candidate = ' '.join(words[i:j])\n",
        "            if candidate not in vocabulary:\n",
        "                continue\n",
        "            candidates.add(candidate)\n",
        "    return candidates\n",
        "\n",
        "def bigram_probability(sentence, bigram_freq):\n",
        "    words = tokenize(sentence)\n",
        "    bigram_prob = 1.0\n",
        "    for i in range(len(words) - 1):\n",
        "        bigram = (words[i], words[i+1])\n",
        "        # Add a small constant to avoid zero probability\n",
        "        bigram_prob *= (bigram_freq[bigram] + 1) / sum(bigram_freq.values())\n",
        "    return bigram_prob\n",
        "\n",
        "def correct_spelling(text, vocabulary, bigram_freq):\n",
        "    words = tokenize(text)\n",
        "    misspellings = find_misspellings(text, vocabulary)\n",
        "\n",
        "    corrected_text = words[:]\n",
        "\n",
        "    for misspelled_word in misspellings:\n",
        "        candidates = get_candidates(misspelled_word, vocabulary)\n",
        "        multi_candidates = get_multigram_candidates(misspelled_word, words, vocabulary)\n",
        "\n",
        "        # Combine single-word and multi-word candidates\n",
        "        all_candidates = candidates.union(multi_candidates)\n",
        "\n",
        "        if all_candidates:\n",
        "            best_candidate = max(\n",
        "                all_candidates,\n",
        "                key=lambda candidate: bigram_probability(\n",
        "                    ' '.join(words).replace(misspelled_word, candidate), bigram_freq\n",
        "                )\n",
        "            )\n",
        "            print(f\"Replacing '{misspelled_word}' with '{best_candidate}'\")  # Print replacement info\n",
        "            corrected_text = [best_candidate if w == misspelled_word else w for w in corrected_text]\n",
        "\n",
        "    return ' '.join(corrected_text)\n",
        "\n",
        "# Example corpus and test case\n",
        "corpus = (\n",
        "    \"The quick brown fox jumps over the lazy dog. \"\n",
        "    \"A journey of a thousand miles begins with a single step. \"\n",
        "    \"To be or not to be, that is the question. \"\n",
        "    \"All that glitters is not gold. \"\n",
        "    \"The only thing we have to fear is fear itself.\"\n",
        ")\n",
        "\n",
        "input_text = \"The quik brown fox jumos ovr the lazy dg. A jurney of a thosand miles begins wth a singel step.\"\n",
        "\n",
        "# Building vocabulary and bigram frequency table\n",
        "vocabulary, bigram_freq = build_vocabulary_and_bigrams(corpus)\n",
        "\n",
        "# Correct the input text\n",
        "corrected_text = correct_spelling(input_text, vocabulary, bigram_freq)\n",
        "print(\"Corrected Text:\", corrected_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McgHe2rmVXzq",
        "outputId": "9eb510ce-6579-4b48-9033-f85e288b0898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'journey', 'over', 'not', 'only', 'itself', 'question', 'all', 'is', 'gold', 'lazy', 'be', 'that', 'fox', 'quick', 'we', 'dog', 'thousand', 'thing', 'begins', 'or', 'have', 'single', 'the', 'with', 'step', 'brown', 'jumps', 'a', 'of', 'to', 'fear', 'miles', 'glitters'}\n",
            "Bigram Frequency Table: {('the', 'quick'): 1, ('quick', 'brown'): 1, ('brown', 'fox'): 1, ('fox', 'jumps'): 1, ('jumps', 'over'): 1, ('over', 'the'): 1, ('the', 'lazy'): 1, ('lazy', 'dog'): 1, ('dog', 'a'): 1, ('a', 'journey'): 1, ('journey', 'of'): 1, ('of', 'a'): 1, ('a', 'thousand'): 1, ('thousand', 'miles'): 1, ('miles', 'begins'): 1, ('begins', 'with'): 1, ('with', 'a'): 1, ('a', 'single'): 1, ('single', 'step'): 1, ('step', 'to'): 1, ('to', 'be'): 2, ('be', 'or'): 1, ('or', 'not'): 1, ('not', 'to'): 1, ('be', 'that'): 1, ('that', 'is'): 1, ('is', 'the'): 1, ('the', 'question'): 1, ('question', 'all'): 1, ('all', 'that'): 1, ('that', 'glitters'): 1, ('glitters', 'is'): 1, ('is', 'not'): 1, ('not', 'gold'): 1, ('gold', 'the'): 1, ('the', 'only'): 1, ('only', 'thing'): 1, ('thing', 'we'): 1, ('we', 'have'): 1, ('have', 'to'): 1, ('to', 'fear'): 1, ('fear', 'is'): 1, ('is', 'fear'): 1, ('fear', 'itself'): 1}\n",
            "Identified Misspellings: ['quik', 'jumos', 'ovr', 'dg', 'jurney', 'thosand', 'wth', 'singel']\n",
            "Candidates for 'quik': {'quick'}\n",
            "Replacing 'quik' with 'quick'\n",
            "Candidates for 'jumos': {'jumps'}\n",
            "Replacing 'jumos' with 'jumps'\n",
            "Candidates for 'ovr': {'over', 'or'}\n",
            "Replacing 'ovr' with 'over'\n",
            "Candidates for 'dg': {'dog'}\n",
            "Replacing 'dg' with 'dog'\n",
            "Candidates for 'jurney': {'journey'}\n",
            "Replacing 'jurney' with 'journey'\n",
            "Candidates for 'thosand': {'thousand'}\n",
            "Replacing 'thosand' with 'thousand'\n",
            "Candidates for 'wth': {'with'}\n",
            "Replacing 'wth' with 'with'\n",
            "Candidates for 'singel': {'single'}\n",
            "Replacing 'singel' with 'single'\n",
            "Corrected Text: the quick brown fox jumps over the lazy dog a journey of a thousand miles begins with a single step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "def tokenize(text):\n",
        "    # Tokenize text into words using regex for simplicity\n",
        "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
        "\n",
        "def build_vocabulary_and_bigrams(corpus):\n",
        "    words = tokenize(corpus)\n",
        "    vocabulary = set(words)\n",
        "\n",
        "    bigram_freq = defaultdict(int)\n",
        "    for i in range(len(words) - 1):\n",
        "        bigram = (words[i], words[i+1])\n",
        "        bigram_freq[bigram] += 1\n",
        "\n",
        "    return vocabulary, bigram_freq\n",
        "\n",
        "def find_misspellings(text, vocabulary):\n",
        "    words = tokenize(text)\n",
        "    return [word for word in words if word not in vocabulary]\n",
        "\n",
        "def edits1(word):\n",
        "    # Generate all words one edit away from `word`\n",
        "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "    deletes = [L + R[1:] for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
        "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
        "    inserts = [L + c + R for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def get_candidates(word, vocabulary):\n",
        "    # Get candidate corrections for the misspelled word\n",
        "    return {w for w in edits1(word) if w in vocabulary}\n",
        "\n",
        "def bigram_probability(sentence, bigram_freq):\n",
        "    words = tokenize(sentence)\n",
        "    bigram_prob = 1.0\n",
        "    for i in range(len(words) - 1):\n",
        "        bigram = (words[i], words[i+1])\n",
        "        # Add a small constant to avoid zero probability\n",
        "        bigram_prob *= (bigram_freq[bigram] + 1) / sum(bigram_freq.values())\n",
        "    return bigram_prob\n",
        "\n",
        "def correct_spelling(text, vocabulary, bigram_freq):\n",
        "    words = tokenize(text)\n",
        "    misspellings = find_misspellings(text, vocabulary)\n",
        "\n",
        "    corrected_text = words[:]\n",
        "\n",
        "    for misspelled_word in misspellings:\n",
        "        candidates = get_candidates(misspelled_word, vocabulary)\n",
        "\n",
        "        if candidates:\n",
        "            best_candidate = max(\n",
        "                candidates,\n",
        "                key=lambda candidate: bigram_probability(\n",
        "                    ' '.join(words).replace(misspelled_word, candidate), bigram_freq\n",
        "                )\n",
        "            )\n",
        "            corrected_text = [best_candidate if w == misspelled_word else w for w in corrected_text]\n",
        "\n",
        "    return ' '.join(corrected_text)\n",
        "\n",
        "# Sample corpus and test case\n",
        "corpus = \"This is a sample corpus for creating a statistical spell checker using bigram model.\"\n",
        "input_text = \"This is a smaple corpuss for creatng a statitical spell cheker.\"\n",
        "\n",
        "# Building vocabulary and bigram frequency table\n",
        "vocabulary, bigram_freq = build_vocabulary_and_bigrams(corpus)\n",
        "\n",
        "# Correct the input text\n",
        "corrected_text = correct_spelling(input_text, vocabulary, bigram_freq)\n",
        "print(\"Corrected Text:\", corrected_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIgB_s5LMREh",
        "outputId": "5c45b202-d469-41b0-fa67-2d95108f3317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrected Text: this is a sample corpus for creating a statistical spell checker\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a text classifier for sentiment analysis using the Naive Bayes theorem. Use Add-k smoothing to handle zero probabilities. Compare the performance of your classifier for k values 0.25, 0.75, and 1."
      ],
      "metadata": {
        "id": "EPIQf6ifMI4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from nltk.corpus import movie_reviews\n",
        "import nltk\n",
        "\n",
        "# Ensure that you have the nltk movie reviews corpus\n",
        "nltk.download('movie_reviews')\n",
        "\n",
        "# Load the IMDb dataset\n",
        "documents = [(movie_reviews.raw(fileid), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(documents, columns=['text', 'label'])\n",
        "\n",
        "# Map the labels to binary values: 'pos' -> 1 and 'neg' -> 0\n",
        "df['label'] = df['label'].map({'pos': 1, 'neg': 0})\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize the text data using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vect = vectorizer.fit_transform(X_train)\n",
        "X_test_vect = vectorizer.transform(X_test)\n",
        "\n",
        "# Naive Bayes Classifier with Add-k smoothing\n",
        "class NaiveBayes:\n",
        "    def __init__(self, alpha=1.0):\n",
        "        self.alpha = alpha  # Add-k smoothing parameter\n",
        "        self.class_priors = {}\n",
        "        self.word_likelihoods = {}\n",
        "        self.vocab_size = 0\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Count the number of documents in each class\n",
        "        n_documents = len(y)\n",
        "        self.vocab_size = X.shape[1]\n",
        "\n",
        "        # Calculate class prior probabilities\n",
        "        classes = np.unique(y)\n",
        "        self.class_priors = {c: np.log(np.sum(y == c) / n_documents) for c in classes}\n",
        "\n",
        "        # Count word occurrences for each class\n",
        "        word_counts = {c: np.zeros(self.vocab_size) for c in classes}\n",
        "        for idx in range(len(y)):\n",
        "            class_label = y.iloc[idx]  # Use iloc to access the index\n",
        "            word_counts[class_label] += X[idx].toarray()[0]  # Access the vectorized data\n",
        "\n",
        "        # Calculate the likelihood of each word given the class with Add-k smoothing\n",
        "        for c in classes:\n",
        "            total_count = np.sum(word_counts[c]) + self.alpha * self.vocab_size  # Total count with smoothing\n",
        "            self.word_likelihoods[c] = (word_counts[c] + self.alpha) / total_count\n",
        "\n",
        "    def predict(self, X):\n",
        "        log_probabilities = []\n",
        "        for c in self.class_priors:\n",
        "            # Calculate log-probability for each class\n",
        "            log_prob = self.class_priors[c] + X.dot(np.log(self.word_likelihoods[c]))\n",
        "            log_probabilities.append(log_prob)\n",
        "\n",
        "        # Choose the class with the highest probability\n",
        "        return np.argmax(log_probabilities, axis=0)\n",
        "\n",
        "def evaluate_model(k_value):\n",
        "    # Create and train the model\n",
        "    model = NaiveBayes(alpha=k_value)\n",
        "    model.fit(X_train_vect, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.predict(X_test_vect)\n",
        "\n",
        "    # Calculate accuracy and report\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    report = classification_report(y_test, predictions)\n",
        "    return accuracy, report\n",
        "\n",
        "# Evaluate for different k values\n",
        "k_values = [0.25, 0.75, 1.0]\n",
        "results = {}\n",
        "\n",
        "for k in k_values:\n",
        "    accuracy, report = evaluate_model(k)\n",
        "    results[k] = (accuracy, report)\n",
        "\n",
        "# Display the results\n",
        "for k, (accuracy, report) in results.items():\n",
        "    print(f\"Results for k={k}:\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}\")\n",
        "    print(f\"Classification Report:\\n{report}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRntBqD3ZcpH",
        "outputId": "14712b5e-092c-4fb5-d6b8-110bea8cb208"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for k=0.25:\n",
            "Accuracy: 0.80\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.86      0.81       199\n",
            "           1       0.84      0.75      0.79       201\n",
            "\n",
            "    accuracy                           0.80       400\n",
            "   macro avg       0.81      0.80      0.80       400\n",
            "weighted avg       0.81      0.80      0.80       400\n",
            "\n",
            "Results for k=0.75:\n",
            "Accuracy: 0.81\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.89      0.82       199\n",
            "           1       0.87      0.73      0.79       201\n",
            "\n",
            "    accuracy                           0.81       400\n",
            "   macro avg       0.82      0.81      0.81       400\n",
            "weighted avg       0.82      0.81      0.81       400\n",
            "\n",
            "Results for k=1.0:\n",
            "Accuracy: 0.81\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.90      0.82       199\n",
            "           1       0.88      0.72      0.79       201\n",
            "\n",
            "    accuracy                           0.81       400\n",
            "   macro avg       0.82      0.81      0.81       400\n",
            "weighted avg       0.82      0.81      0.81       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_plural_noun_accepted_fsa(word):\n",
        "    if len(word) < 2 or word[-1] != 's':\n",
        "        return False\n",
        "\n",
        "    word = word[::-1]\n",
        "    state = 'S1'\n",
        "\n",
        "    for char in word[1:]:\n",
        "        if state == 'S1':\n",
        "            if char == 'y':\n",
        "                state = 'S2'\n",
        "            elif char == 'e':\n",
        "                state = 'S3'\n",
        "            else:\n",
        "                return False\n",
        "        elif state == 'S2':\n",
        "            if char in 'aeiou':\n",
        "                state = 'S5'\n",
        "            else:\n",
        "                return False\n",
        "        elif state == 'S3':\n",
        "            if char == 'i':\n",
        "                state = 'S4'\n",
        "            else:\n",
        "                return False\n",
        "        elif state == 'S4':\n",
        "            if char.isalpha() and char not in 'aeiou':\n",
        "                state = 'S6'\n",
        "            else:\n",
        "                return False\n",
        "        elif state == 'S5':\n",
        "            continue\n",
        "        elif state == 'S6':\n",
        "            continue\n",
        "\n",
        "    return True\n",
        "\n",
        "test_words = ['boys', 'toys', 'ponies', 'skies', 'puppies', 'boies', 'toies', 'ponys', 'carries', 'daisies']\n",
        "results = {word: is_plural_noun_accepted_fsa(word) for word in test_words}\n",
        "\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aLrR2Adbzv_",
        "outputId": "4c463597-92c8-425a-ab3e-fc208cc16906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'boys': True, 'toys': True, 'ponies': True, 'skies': True, 'puppies': True, 'boies': False, 'toies': False, 'ponys': False, 'carries': True, 'daisies': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_plural_noun_accepted_fsa(word):\n",
        "    # Check if the word is too short or doesn't end with 's'\n",
        "    if len(word) < 2 or word[-1] != 's':\n",
        "        return False\n",
        "\n",
        "    # Reverse the word for processing\n",
        "    word = word[::-1]\n",
        "    state = 'S1'\n",
        "\n",
        "    for char in word[1:]:\n",
        "        if state == 'S1':\n",
        "            if char == 'y':\n",
        "                state = 'S2'\n",
        "            elif char == 'e':\n",
        "                state = 'S3'\n",
        "            else:\n",
        "                return False\n",
        "        elif state == 'S2':\n",
        "            if char in 'aeiou':\n",
        "                state = 'S5'\n",
        "            else:\n",
        "                return False\n",
        "        elif state == 'S3':\n",
        "            if char == 'i':\n",
        "                state = 'S4'\n",
        "            else:\n",
        "                return False\n",
        "        elif state == 'S4':\n",
        "            if char.isalpha() and char not in 'aeiou':\n",
        "                state = 'S6'\n",
        "            else:\n",
        "                return False\n",
        "        elif state == 'S5':\n",
        "            continue  # Stay in S5 if we encounter a vowel\n",
        "        elif state == 'S6':\n",
        "            continue  # Stay in S6 if we encounter a consonant\n",
        "\n",
        "    return True\n",
        "\n",
        "# List of test words\n",
        "test_words = ['boys', 'toys', 'ponies', 'skies', 'puppies', 'boies', 'toies', 'ponys', 'carries', 'daisies']\n",
        "\n",
        "# Create a dictionary to store the results of the test words\n",
        "results = {word: is_plural_noun_accepted_fsa(word) for word in test_words}\n",
        "\n",
        "# Print the results\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJlzvFopkkD5",
        "outputId": "17b32b61-f6d1-4fff-e47c-e2ac7b42677f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'boys': True, 'toys': True, 'ponies': True, 'skies': True, 'puppies': True, 'boies': False, 'toies': False, 'ponys': False, 'carries': True, 'daisies': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7"
      ],
      "metadata": {
        "id": "bl-a5CDRgHMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Transition probabilities (a_ij)\n",
        "transition_probs = {\n",
        "    'START': {'NN': 0.5, 'VB': 0.25, 'JJ': 0.25, 'RB': 0, 'STOP': 0},\n",
        "    'NN': {'NN': 0.25, 'VB': 0.5, 'JJ': 0, 'RB': 0.25, 'STOP': 0},\n",
        "    'VB': {'NN': 0.25, 'VB': 0, 'JJ': 0.25, 'RB': 0.25, 'STOP': 0.25},\n",
        "    'JJ': {'NN': 0.75, 'VB': 0, 'JJ': 0.25, 'RB': 0, 'STOP': 0},\n",
        "    'RB': {'NN': 0.25, 'VB': 0, 'JJ': 0.25, 'RB': 0, 'STOP': 0.5},\n",
        "}\n",
        "\n",
        "# Emission probabilities (b_ik)\n",
        "emission_probs = {\n",
        "    'NN': {'time': 0.1, 'flies': 0.01, 'fast': 0.01},\n",
        "    'VB': {'time': 0.01, 'flies': 0.1, 'fast': 0.01},\n",
        "    'JJ': {'time': 0, 'flies': 0, 'fast': 0.1},\n",
        "    'RB': {'time': 0, 'flies': 0, 'fast': 0.1},\n",
        "}\n",
        "\n",
        "# List of POS tags and words\n",
        "tags = ['NN', 'VB', 'JJ', 'RB']\n",
        "sentence = ['time', 'flies', 'fast']\n",
        "\n",
        "# Initialize Viterbi and backpointer tables\n",
        "viterbi = [{} for _ in range(len(sentence))]\n",
        "backpointer = [{} for _ in range(len(sentence))]\n",
        "\n",
        "# Print initial sentence\n",
        "print(\"Sentence:\", sentence)\n",
        "\n",
        "# Initialize base case for t = 0\n",
        "print(\"\\nStep 1: Initialization\")\n",
        "for tag in tags:\n",
        "    viterbi[0][tag] = transition_probs['START'][tag] * emission_probs[tag].get(sentence[0], 0)\n",
        "    backpointer[0][tag] = None\n",
        "    print(f\"viterbi[0][{tag}] = START->{tag} * emission[{tag}][{sentence[0]}] = \"\n",
        "          f\"{transition_probs['START'][tag]} * {emission_probs[tag].get(sentence[0], 0)} = {viterbi[0][tag]}\")\n",
        "\n",
        "# Recursive case for t > 0\n",
        "for t in range(1, len(sentence)):\n",
        "    print(f\"\\nStep 2.{t}: Recursion for word '{sentence[t]}'\")\n",
        "    for tag in tags:\n",
        "        max_prob, prev_tag = max(\n",
        "            (viterbi[t - 1][prev] * transition_probs[prev][tag] * emission_probs[tag].get(sentence[t], 0), prev)\n",
        "            for prev in tags\n",
        "        )\n",
        "        viterbi[t][tag] = max_prob\n",
        "        backpointer[t][tag] = prev_tag\n",
        "        print(f\"viterbi[{t}][{tag}] = max(prev_tag->{tag}) * emission[{tag}][{sentence[t]}] = {max_prob}, \"\n",
        "              f\"coming from {prev_tag}\")\n",
        "\n",
        "# Termination step\n",
        "print(\"\\nStep 3: Termination\")\n",
        "max_final_prob, final_tag = max(\n",
        "    (viterbi[len(sentence) - 1][tag] * transition_probs[tag]['STOP'], tag) for tag in tags\n",
        ")\n",
        "print(f\"Final probabilities multiplied with STOP: {[(tag, viterbi[len(sentence) - 1][tag] * transition_probs[tag]['STOP']) for tag in tags]}\")\n",
        "print(f\"Max final probability is {max_final_prob}, with final tag '{final_tag}'\")\n",
        "\n",
        "# Traceback to find the best path\n",
        "print(\"\\nStep 4: Traceback\")\n",
        "best_path = []\n",
        "current_tag = final_tag\n",
        "for t in range(len(sentence) - 1, -1, -1):\n",
        "    best_path.insert(0, current_tag)\n",
        "    print(f\"Backtracking at position {t}: Current tag = {current_tag}, Backpointer = {backpointer[t][current_tag]}\")\n",
        "    current_tag = backpointer[t][current_tag]\n",
        "\n",
        "# Output the best path\n",
        "print(\"\\nMost probable POS tag sequence:\", best_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2m-bjWB3gE9B",
        "outputId": "30db8839-ff2b-4c97-97af-130935096338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: ['time', 'flies', 'fast']\n",
            "\n",
            "Step 1: Initialization\n",
            "viterbi[0][NN] = START->NN * emission[NN][time] = 0.5 * 0.1 = 0.05\n",
            "viterbi[0][VB] = START->VB * emission[VB][time] = 0.25 * 0.01 = 0.0025\n",
            "viterbi[0][JJ] = START->JJ * emission[JJ][time] = 0.25 * 0 = 0.0\n",
            "viterbi[0][RB] = START->RB * emission[RB][time] = 0 * 0 = 0\n",
            "\n",
            "Step 2.1: Recursion for word 'flies'\n",
            "viterbi[1][NN] = max(prev_tag->NN) * emission[NN][flies] = 0.000125, coming from NN\n",
            "viterbi[1][VB] = max(prev_tag->VB) * emission[VB][flies] = 0.0025000000000000005, coming from NN\n",
            "viterbi[1][JJ] = max(prev_tag->JJ) * emission[JJ][flies] = 0.0, coming from VB\n",
            "viterbi[1][RB] = max(prev_tag->RB) * emission[RB][flies] = 0.0, coming from VB\n",
            "\n",
            "Step 2.2: Recursion for word 'fast'\n",
            "viterbi[2][NN] = max(prev_tag->NN) * emission[NN][fast] = 6.250000000000001e-06, coming from VB\n",
            "viterbi[2][VB] = max(prev_tag->VB) * emission[VB][fast] = 6.25e-07, coming from NN\n",
            "viterbi[2][JJ] = max(prev_tag->JJ) * emission[JJ][fast] = 6.250000000000001e-05, coming from VB\n",
            "viterbi[2][RB] = max(prev_tag->RB) * emission[RB][fast] = 6.250000000000001e-05, coming from VB\n",
            "\n",
            "Step 3: Termination\n",
            "Final probabilities multiplied with STOP: [('NN', 0.0), ('VB', 1.5625e-07), ('JJ', 0.0), ('RB', 3.125000000000001e-05)]\n",
            "Max final probability is 3.125000000000001e-05, with final tag 'RB'\n",
            "\n",
            "Step 4: Traceback\n",
            "Backtracking at position 2: Current tag = RB, Backpointer = VB\n",
            "Backtracking at position 1: Current tag = VB, Backpointer = NN\n",
            "Backtracking at position 0: Current tag = NN, Backpointer = None\n",
            "\n",
            "Most probable POS tag sequence: ['NN', 'VB', 'RB']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8"
      ],
      "metadata": {
        "id": "uZz73RG3hx_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, Counter\n",
        "import math\n",
        "\n",
        "# Step 1: Define a sample corpus\n",
        "corpus = [\n",
        "    \"Natural language processing is a fascinating field.\",\n",
        "    \"It involves the interaction between computers and human language.\",\n",
        "    \"One important aspect is understanding context.\",\n",
        "    \"Another key element is generating coherent responses.\",\n",
        "    \"Machine learning plays a crucial role in NLP.\",\n",
        "    \"Deep learning models like transformers are widely used.\",\n",
        "    \"Tokenization is an essential preprocessing step.\",\n",
        "    \"Named entity recognition is another core task.\",\n",
        "    \"Sentiment analysis helps understand emotions in text.\",\n",
        "    \"Language modeling is fundamental to many NLP applications.\"\n",
        "]\n",
        "\n",
        "# Preprocessing: Tokenize sentences and add START and STOP tokens\n",
        "def preprocess_corpus(corpus):\n",
        "    tokenized_corpus = []\n",
        "    for sentence in corpus:\n",
        "        words = sentence.lower().replace('.', '').split()\n",
        "        tokenized_corpus.append([\"<START>\"] + words + [\"<STOP>\"])\n",
        "    return tokenized_corpus\n",
        "\n",
        "tokenized_corpus = preprocess_corpus(corpus)\n",
        "\n",
        "# Step 2: Create a bigram model\n",
        "def build_bigram_model(tokenized_corpus):\n",
        "    bigram_counts = defaultdict(Counter)\n",
        "    unigram_counts = Counter()\n",
        "\n",
        "    for sentence in tokenized_corpus:\n",
        "        for i in range(len(sentence) - 1):\n",
        "            bigram_counts[sentence[i]][sentence[i + 1]] += 1\n",
        "            unigram_counts[sentence[i]] += 1\n",
        "        unigram_counts[sentence[-1]] += 1  # For the last token (STOP)\n",
        "\n",
        "    bigram_probs = defaultdict(dict)\n",
        "    for word1 in bigram_counts:\n",
        "        for word2 in bigram_counts[word1]:\n",
        "            bigram_probs[word1][word2] = bigram_counts[word1][word2] / unigram_counts[word1]\n",
        "\n",
        "    return bigram_probs\n",
        "\n",
        "bigram_model = build_bigram_model(tokenized_corpus)\n",
        "\n",
        "# Step 3: Calculate the probability of a given sentence\n",
        "def calculate_sentence_probability(sentence, bigram_model):\n",
        "    words = [\"<START>\"] + sentence.lower().replace('.', '').split() + [\"<STOP>\"]\n",
        "    probability = 0  # Use log probabilities to prevent underflow\n",
        "\n",
        "    for i in range(len(words) - 1):\n",
        "        word1, word2 = words[i], words[i + 1]\n",
        "        if word2 in bigram_model[word1]:\n",
        "            probability += math.log(bigram_model[word1][word2])\n",
        "        else:\n",
        "            probability += math.log(1e-10)  # Smoothing for unseen bigrams\n",
        "\n",
        "    return math.exp(probability)  # Convert back from log-probabilities\n",
        "\n",
        "# Example sentence and its probability\n",
        "test_sentence = \"Natural language processing is fascinating.\"\n",
        "sentence_probability = calculate_sentence_probability(test_sentence, bigram_model)\n",
        "\n",
        "# Output results\n",
        "print(\"Bigram Model (Partial):\")\n",
        "for word, transitions in list(bigram_model.items())[:5]:\n",
        "    print(f\"{word}: {transitions}\")\n",
        "\n",
        "print(\"\\nTest Sentence:\", test_sentence)\n",
        "print(\"Probability of the sentence:\", sentence_probability)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlTj-lung6wT",
        "outputId": "93684dfa-ce18-4df5-c437-0c92ffbcf5dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram Model (Partial):\n",
            "<START>: {'natural': 0.1, 'it': 0.1, 'one': 0.1, 'another': 0.1, 'machine': 0.1, 'deep': 0.1, 'tokenization': 0.1, 'named': 0.1, 'sentiment': 0.1, 'language': 0.1}\n",
            "natural: {'language': 1.0}\n",
            "language: {'processing': 0.3333333333333333, '<STOP>': 0.3333333333333333, 'modeling': 0.3333333333333333}\n",
            "processing: {'is': 1.0}\n",
            "is: {'a': 0.16666666666666666, 'understanding': 0.16666666666666666, 'generating': 0.16666666666666666, 'an': 0.16666666666666666, 'another': 0.16666666666666666, 'fundamental': 0.16666666666666666}\n",
            "\n",
            "Test Sentence: Natural language processing is fascinating.\n",
            "Probability of the sentence: 3.333333333333347e-22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10"
      ],
      "metadata": {
        "id": "DjZx4XCBhzKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "# Step 1: Define a corpus of long documents\n",
        "documents = [\n",
        "    \"Natural language processing enables computers to understand and generate human language.\",\n",
        "    \"Deep learning models like transformers have revolutionized natural language processing.\",\n",
        "    \"Tokenization and preprocessing are critical steps in NLP pipelines.\",\n",
        "    \"Named entity recognition identifies entities such as names, dates, and locations in text.\",\n",
        "    \"Sentiment analysis detects emotions in customer reviews or social media data.\",\n",
        "    \"Language modeling predicts the next word in a sequence, a fundamental task in NLP.\",\n",
        "    \"Vector embeddings represent words and documents as dense numerical arrays.\",\n",
        "    \"NLP applications range from chatbots to machine translation and summarization.\",\n",
        "    \"Context is essential for understanding ambiguous language and idiomatic expressions.\",\n",
        "    \"Grammar correction and text generation are other key areas of natural language processing.\"\n",
        "]\n",
        "\n",
        "# Preprocessing: Tokenize the corpus and count word co-occurrences\n",
        "def tokenize_and_count(documents):\n",
        "    vocab = set()\n",
        "    word_counts = []\n",
        "    for doc in documents:\n",
        "        tokens = doc.lower().replace('.', '').split()\n",
        "        vocab.update(tokens)\n",
        "        word_counts.append(tokens)\n",
        "    return list(vocab), word_counts\n",
        "\n",
        "vocab, word_counts = tokenize_and_count(documents)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Create word-to-index mapping\n",
        "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "# Step 2: Compute co-occurrence matrix\n",
        "def build_cooccurrence_matrix(word_counts, vocab_size, window_size=2):\n",
        "    cooccurrence_matrix = np.zeros((vocab_size, vocab_size))\n",
        "    for tokens in word_counts:\n",
        "        for i, word in enumerate(tokens):\n",
        "            word_index = word_to_index[word]\n",
        "            for j in range(max(i - window_size, 0), min(i + window_size + 1, len(tokens))):\n",
        "                if i != j:\n",
        "                    context_word_index = word_to_index[tokens[j]]\n",
        "                    cooccurrence_matrix[word_index][context_word_index] += 1\n",
        "    return cooccurrence_matrix\n",
        "\n",
        "cooccurrence_matrix = build_cooccurrence_matrix(word_counts, vocab_size)\n",
        "\n",
        "# Step 3: Compute PPMI matrix\n",
        "def compute_ppmi_matrix(cooccurrence_matrix):\n",
        "    total_count = np.sum(cooccurrence_matrix)\n",
        "    word_totals = np.sum(cooccurrence_matrix, axis=1)\n",
        "    ppmi_matrix = np.zeros_like(cooccurrence_matrix)\n",
        "    for i in range(cooccurrence_matrix.shape[0]):\n",
        "        for j in range(cooccurrence_matrix.shape[1]):\n",
        "            if cooccurrence_matrix[i][j] > 0:\n",
        "                p_ij = cooccurrence_matrix[i][j] / total_count\n",
        "                p_i = word_totals[i] / total_count\n",
        "                p_j = word_totals[j] / total_count\n",
        "                ppmi = max(0, np.log2(p_ij / (p_i * p_j)))\n",
        "                ppmi_matrix[i][j] = ppmi\n",
        "    return ppmi_matrix\n",
        "\n",
        "ppmi_matrix = compute_ppmi_matrix(cooccurrence_matrix)\n",
        "\n",
        "# Step 4: Compute cosine similarity\n",
        "def calculate_cosine_similarity(ppmi_matrix, word1, word2):\n",
        "    index1, index2 = word_to_index[word1], word_to_index[word2]\n",
        "    vector1, vector2 = ppmi_matrix[index1], ppmi_matrix[index2]\n",
        "    return cosine_similarity([vector1], [vector2])[0][0]\n",
        "\n",
        "# Step 5: Cosine similarity between documents\n",
        "def document_vector(document, ppmi_matrix, word_to_index):\n",
        "    vector = np.zeros(ppmi_matrix.shape[0])\n",
        "    tokens = document.lower().replace('.', '').split()\n",
        "    for token in tokens:\n",
        "        if token in word_to_index:\n",
        "            vector += ppmi_matrix[word_to_index[token]]\n",
        "    return vector\n",
        "\n",
        "def calculate_document_similarity(doc1, doc2, ppmi_matrix, word_to_index):\n",
        "    vec1 = document_vector(doc1, ppmi_matrix, word_to_index)\n",
        "    vec2 = document_vector(doc2, ppmi_matrix, word_to_index)\n",
        "    return cosine_similarity([vec1], [vec2])[0][0]\n",
        "\n",
        "# Example Usage\n",
        "word1 = \"language\"\n",
        "word2 = \"processing\"\n",
        "similarity_words = calculate_cosine_similarity(ppmi_matrix, word1, word2)\n",
        "\n",
        "doc1 = documents[0]\n",
        "doc2 = documents[1]\n",
        "similarity_docs = calculate_document_similarity(doc1, doc2, ppmi_matrix, word_to_index)\n",
        "\n",
        "# Output\n",
        "print(\"PPMI Matrix (Partial):\")\n",
        "print(ppmi_matrix[:5, :5])  # Display part of the matrix for brevity\n",
        "print(f\"\\nCosine Similarity between words '{word1}' and '{word2}': {similarity_words}\")\n",
        "print(f\"Cosine Similarity between documents:\\n'{doc1}'\\nand\\n'{doc2}': {similarity_docs}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CZl4CsgjwAl",
        "outputId": "da55122f-8f2c-4c76-e59a-95c5fe69a1a4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PPMI Matrix (Partial):\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "\n",
            "Cosine Similarity between words 'language' and 'processing': 0.2980892233002522\n",
            "Cosine Similarity between documents:\n",
            "'Natural language processing enables computers to understand and generate human language.'\n",
            "and\n",
            "'Deep learning models like transformers have revolutionized natural language processing.': 0.3750746564962848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Example documents\n",
        "doc1 = \"\"\"\n",
        "Natural language processing (NLP) is a subfield of artificial intelligence.\n",
        "It focuses on the interaction between computers and human language.\n",
        "Applications of NLP include text classification, sentiment analysis, and machine translation.\n",
        "Preprocessing is an important step in NLP pipelines, involving tokenization and normalization.\n",
        "Word embeddings like Word2Vec and GloVe are widely used in NLP tasks.\n",
        "Named entity recognition (NER) is a common NLP task for identifying proper nouns.\n",
        "Another task is part-of-speech tagging, which assigns grammatical roles to words.\n",
        "NLP powers chatbots and virtual assistants like Alexa and Siri.\n",
        "Language models such as GPT are pivotal in generating human-like text.\n",
        "NLP continues to evolve with advancements in deep learning and transformers.\n",
        "\"\"\"\n",
        "\n",
        "doc2 = \"\"\"\n",
        "Computational linguistics is an interdisciplinary field combining linguistics and computer science.\n",
        "It studies how to use computational techniques to analyze and process human language.\n",
        "The field includes syntax, semantics, and phonetics.\n",
        "Applications include automatic speech recognition and text-to-speech systems.\n",
        "Another major area is machine translation, helping translate text between languages.\n",
        "Corpus linguistics uses computational methods to analyze large datasets of text.\n",
        "Dependency parsing is a task in computational linguistics to map sentence structure.\n",
        "Morphological analysis deals with understanding word formation.\n",
        "The goal of computational linguistics is to bridge the gap between natural and machine languages.\n",
        "Computational approaches continue to shape the future of linguistics research.\n",
        "\"\"\"\n",
        "\n",
        "doc3 = \"\"\"\n",
        "Cricket is a popular sport played between two teams of eleven players each.\n",
        "The game is played with a bat and ball on a 22-yard pitch.\n",
        "A cricket match can be of various formats, including Test, ODI, and T20.\n",
        "The bowler delivers the ball, and the batsman tries to score runs.\n",
        "Fielding is a key aspect of cricket, involving catching and stopping the ball.\n",
        "The ICC (International Cricket Council) governs the sport globally.\n",
        "Cricketers like Sachin Tendulkar and Virat Kohli have become household names.\n",
        "Each team has a captain who strategizes and decides the batting order.\n",
        "The umpire ensures fair play during the match.\n",
        "Cricket has a massive following, especially in countries like India, Australia, and England.\n",
        "\"\"\"\n",
        "\n",
        "documents = [doc1, doc2, doc3]\n",
        "\n",
        "# Step 1: Preprocess and build a vocabulary\n",
        "def preprocess(doc):\n",
        "    tokens = doc.lower().replace('.', '').replace(',', '').split()\n",
        "    return tokens\n",
        "\n",
        "tokenized_docs = [preprocess(doc) for doc in documents]\n",
        "vocabulary = list(set(word for doc in tokenized_docs for word in doc))\n",
        "word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
        "\n",
        "# Step 2: Build co-occurrence matrix\n",
        "def build_cooccurrence_matrix(tokenized_docs, window_size=2):\n",
        "    vocab_size = len(vocabulary)\n",
        "    cooccurrence_matrix = np.zeros((vocab_size, vocab_size))\n",
        "\n",
        "    for doc in tokenized_docs:\n",
        "        for i, word in enumerate(doc):\n",
        "            word_idx = word_to_index[word]\n",
        "            # Consider a window around the word\n",
        "            for j in range(max(0, i - window_size), min(len(doc), i + window_size + 1)):\n",
        "                if i != j:\n",
        "                    context_word_idx = word_to_index[doc[j]]\n",
        "                    cooccurrence_matrix[word_idx][context_word_idx] += 1\n",
        "    return cooccurrence_matrix\n",
        "\n",
        "cooccurrence_matrix = build_cooccurrence_matrix(tokenized_docs)\n",
        "\n",
        "# Step 3: Compute PPMI matrix\n",
        "def compute_ppmi_matrix(cooccurrence_matrix):\n",
        "    total_count = np.sum(cooccurrence_matrix)\n",
        "    word_counts = np.sum(cooccurrence_matrix, axis=1)\n",
        "    ppmi_matrix = np.zeros_like(cooccurrence_matrix)\n",
        "\n",
        "    for i in range(cooccurrence_matrix.shape[0]):\n",
        "        for j in range(cooccurrence_matrix.shape[1]):\n",
        "            if cooccurrence_matrix[i][j] > 0:\n",
        "                p_ij = cooccurrence_matrix[i][j] / total_count\n",
        "                p_i = word_counts[i] / total_count\n",
        "                p_j = word_counts[j] / total_count\n",
        "                ppmi = max(np.log2(p_ij / (p_i * p_j)), 0)\n",
        "                ppmi_matrix[i][j] = ppmi\n",
        "    return ppmi_matrix\n",
        "\n",
        "ppmi_matrix = compute_ppmi_matrix(cooccurrence_matrix)\n",
        "\n",
        "# Step 4: Compute cosine similarity\n",
        "def calculate_cosine_similarity(vector1, vector2):\n",
        "    return cosine_similarity(vector1.reshape(1, -1), vector2.reshape(1, -1))[0][0]\n",
        "\n",
        "# Step 5: Example usage\n",
        "# Cosine similarity between two documents\n",
        "doc_vectors = [np.sum(ppmi_matrix[[word_to_index[word] for word in doc if word in word_to_index]], axis=0) for doc in tokenized_docs]\n",
        "doc_similarity = calculate_cosine_similarity(doc_vectors[0], doc_vectors[1])\n",
        "print(f\"Cosine similarity between Doc1 and Doc2: {doc_similarity:.4f}\")\n",
        "\n",
        "# Cosine similarity between two words\n",
        "word1, word2 = \"nlp\", \"language\"\n",
        "if word1 in word_to_index and word2 in word_to_index:\n",
        "    word1_vector = ppmi_matrix[word_to_index[word1]]\n",
        "    word2_vector = ppmi_matrix[word_to_index[word2]]\n",
        "    word_similarity = calculate_cosine_similarity(word1_vector, word2_vector)\n",
        "    print(f\"Cosine similarity between '{word1}' and '{word2}': {word_similarity:.4f}\")\n",
        "else:\n",
        "    print(f\"One or both words not in vocabulary: {word1}, {word2}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDPt6fdkh0xw",
        "outputId": "654c87ae-9e19-4753-c2b8-759edf7d4e74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between Doc1 and Doc2: 0.6515\n",
            "Cosine similarity between 'nlp' and 'language': 0.0542\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Step 1: Define a corpus of training documents\n",
        "documents = [\n",
        "    \"Natural language processing enables computers to understand and generate human language.\",\n",
        "    \"Deep learning models like transformers have revolutionized natural language processing.\",\n",
        "    \"Tokenization and preprocessing are critical steps in NLP pipelines.\",\n",
        "    \"Named entity recognition identifies entities such as names, dates, and locations in text.\",\n",
        "    \"Sentiment analysis detects emotions in customer reviews or social media data.\",\n",
        "    \"Language modeling predicts the next word in a sequence, a fundamental task in NLP.\",\n",
        "    \"Vector embeddings represent words and documents as dense numerical arrays.\",\n",
        "    \"NLP applications range from chatbots to machine translation and summarization.\",\n",
        "    \"Context is essential for understanding ambiguous language and idiomatic expressions.\",\n",
        "    \"Grammar correction and text generation are other key areas of natural language processing.\"\n",
        "]\n",
        "\n",
        "# Step 2: Compute the TF-IDF matrix\n",
        "def compute_tfidf_matrix(documents):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    return tfidf_matrix, feature_names\n",
        "\n",
        "tfidf_matrix, feature_names = compute_tfidf_matrix(documents)\n",
        "\n",
        "# Step 3: Cosine similarity between two documents\n",
        "def calculate_document_similarity(doc1_index, doc2_index, tfidf_matrix):\n",
        "    vector1 = tfidf_matrix[doc1_index]\n",
        "    vector2 = tfidf_matrix[doc2_index]\n",
        "    similarity = cosine_similarity(vector1, vector2)[0][0]\n",
        "    return similarity\n",
        "\n",
        "# Step 4: Cosine similarity between two words\n",
        "def calculate_word_similarity(word1, word2, tfidf_matrix, feature_names):\n",
        "    try:\n",
        "        index1 = feature_names.tolist().index(word1)\n",
        "        index2 = feature_names.tolist().index(word2)\n",
        "        word_vector1 = tfidf_matrix[:, index1].toarray().flatten()\n",
        "        word_vector2 = tfidf_matrix[:, index2].toarray().flatten()\n",
        "        similarity = cosine_similarity([word_vector1], [word_vector2])[0][0]\n",
        "        return similarity\n",
        "    except ValueError:\n",
        "        return \"One or both words not found in the vocabulary.\"\n",
        "\n",
        "# Example Usage\n",
        "# Cosine similarity between two documents\n",
        "doc1_index = 0  # First document\n",
        "doc2_index = 1  # Second document\n",
        "similarity_docs = calculate_document_similarity(doc1_index, doc2_index, tfidf_matrix)\n",
        "\n",
        "# Cosine similarity between two words\n",
        "word1 = \"language\"\n",
        "word2 = \"processing\"\n",
        "similarity_words = calculate_word_similarity(word1, word2, tfidf_matrix, feature_names)\n",
        "\n",
        "# Output results\n",
        "print(\"TF-IDF Matrix (Partial):\")\n",
        "print(tfidf_matrix.toarray()[:5, :5])  # Show partial matrix for brevity\n",
        "print(f\"\\nFeature Names (Partial): {feature_names[:5]}\")\n",
        "print(f\"\\nCosine Similarity between documents {doc1_index} and {doc2_index}: {similarity_docs}\")\n",
        "print(f\"Cosine Similarity between words '{word1}' and '{word2}': {similarity_words}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCXJZiK-kDVh",
        "outputId": "af71d4f1-99e9-4618-dcc8-4b0afa85b705"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Matrix (Partial):\n",
            "[[0.         0.         0.16742341 0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.18489537 0.         0.32244345]\n",
            " [0.         0.         0.14617824 0.         0.        ]\n",
            " [0.         0.30953339 0.         0.         0.        ]]\n",
            "\n",
            "Feature Names (Partial): ['ambiguous' 'analysis' 'and' 'applications' 'are']\n",
            "\n",
            "Cosine Similarity between documents 0 and 1: 0.21392570912457406\n",
            "Cosine Similarity between words 'language' and 'processing': 0.8252940664534878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# Training data and labels\n",
        "train_data = [\n",
        "    (\"I love fish. The smoked bass fish was delicious.\", \"fish\"),\n",
        "    (\"The bass fish swam along the line.\", \"fish\"),\n",
        "    (\"He hauled in a big catch of smoked bass fish.\", \"fish\"),\n",
        "    (\"The bass guitar player played a smooth jazz line.\", \"guitar\")\n",
        "]\n",
        "\n",
        "# Vocabulary and preprocessing\n",
        "vocabulary = set()\n",
        "word_counts = {'fish': defaultdict(int), 'guitar': defaultdict(int)}\n",
        "class_counts = {'fish': 0, 'guitar': 0}\n",
        "total_words = {'fish': 0, 'guitar': 0}\n",
        "\n",
        "# Preprocess the training data to build vocabulary and counts\n",
        "for sentence, label in train_data:\n",
        "    words = sentence.lower().split()\n",
        "    class_counts[label] += 1\n",
        "    total_words[label] += len(words)\n",
        "\n",
        "    for word in words:\n",
        "        word_counts[label][word] += 1\n",
        "        vocabulary.add(word)\n",
        "\n",
        "# Add-1 smoothing\n",
        "V = len(vocabulary)  # Vocabulary size\n",
        "total_sentences = len(train_data)  # Total number of sentences\n",
        "\n",
        "# Calculate priors and conditional probabilities with add-1 smoothing\n",
        "def calculate_probabilities(class_counts, word_counts, total_words, V):\n",
        "    prior_probs = {label: class_counts[label] / total_sentences for label in class_counts}\n",
        "    cond_probs = {label: {} for label in class_counts}\n",
        "\n",
        "    for label in class_counts:\n",
        "        total_word_count_in_class = total_words[label]\n",
        "\n",
        "        for word in vocabulary:\n",
        "            cond_probs[label][word] = (word_counts[label].get(word, 0) + 1) / (total_word_count_in_class + V)\n",
        "\n",
        "    return prior_probs, cond_probs\n",
        "\n",
        "prior_probs, cond_probs = calculate_probabilities(class_counts, word_counts, total_words, V)\n",
        "\n",
        "# Test sentence and test word\n",
        "test_sentence = \"He loves jazz. The bass line provided the foundation for the guitar solo in the jazz piece\"\n",
        "test_word = \"bass\"\n",
        "\n",
        "# Process test sentence\n",
        "test_words = test_sentence.lower().split()\n",
        "\n",
        "# Calculate posterior probabilities for each class (fish, guitar)\n",
        "def classify(test_words, prior_probs, cond_probs, V):\n",
        "    post_probs = {}\n",
        "\n",
        "    for label in prior_probs:\n",
        "        prob = np.log(prior_probs[label])\n",
        "\n",
        "        for word in test_words:\n",
        "            prob += np.log(cond_probs[label].get(word, 1 / (V + total_words[label])))\n",
        "\n",
        "        post_probs[label] = prob\n",
        "\n",
        "    # Return the class with the highest posterior probability\n",
        "    return max(post_probs, key=post_probs.get)\n",
        "\n",
        "predicted_sense = classify(test_words, prior_probs, cond_probs, V)\n",
        "\n",
        "print(f\"Predicted sense for the word '{test_word}' in the test sentence: {predicted_sense}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdcPLvNlkyg-",
        "outputId": "50d7dbf9-97a5-4c57-c4ad-2ade7176edf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted sense for the word 'bass' in the test sentence: guitar\n"
          ]
        }
      ]
    }
  ]
}